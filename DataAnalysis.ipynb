{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3716db0-1beb-4f87-bb7b-a8359904ebd4",
   "metadata": {},
   "source": [
    "First, we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc258bd-c89b-4b46-90c4-fc7bdd8767bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "pd.options.display.max_rows = 3\n",
    "\n",
    "metadata_df = pd.read_csv('src/data/metadata.csv')\n",
    "word_hist_union_raw_df = pd.read_csv('src/data/features/word_histogram_union_raw.csv')\n",
    "word_hist_union_pruned_df = pd.read_csv('src/data/features/word_histogram_union_pruned.csv')\n",
    "word_hist_inter_raw_df = pd.read_csv('src/data/features/word_histogram_inter_raw.csv')\n",
    "word_hist_inter_pruned_df = pd.read_csv('src/data/features/word_histogram_inter_pruned.csv')\n",
    "\n",
    "# Find all authors\n",
    "#list_of_authors = list(set(metadata_df['first_author']))\n",
    "#print(list_of_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85e4d4-a378-4fdc-92ac-05d1e10927e4",
   "metadata": {},
   "source": [
    "Cleaning the data by merging data of aliases of one author:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15b082-7cde-4cb3-8134-15b85a5cbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_authors(author_name, alias):\n",
    "    \"\"\"\n",
    "    In case one author has multiple aliases, merge the data for this author, so all papers are under the name given by author_name\n",
    "    Example: 'Florentin Millour' and 'F. Millour' are falsely listed as one author\n",
    "    Also works with two lists of same length, e.g.\n",
    "    merge_authors(['Florentin Millour', 'James Leftley'], ['F. Millour', 'J. Leftley'])\n",
    "    \"\"\"\n",
    "    new_metadata_df = metadata_df.replace(alias, author_name)\n",
    "    return new_metadata_df\n",
    "\n",
    "metadata_df = merge_authors('Florentin Millour', 'F. Millour')\n",
    "\n",
    "list_of_authors = list(set(metadata_df['first_author']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9823d-620b-4460-896f-a05da9d0800d",
   "metadata": {},
   "source": [
    "We should also check the total word count of each author and remove authors with no data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b2180-d006-4a41-93f2-4a0f1dadbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_ids_for_author(author):\n",
    "    paper_ids = list(metadata_df.query(f\"first_author=='{author}'\")[\"arxiv_id\"])\n",
    "    return paper_ids\n",
    "\n",
    "def get_word_count_for_paper(paper_id, words='all'):\n",
    "    \"\"\"\n",
    "        Gets the total word count for a list of words, if 'all' it will get the total word count of this paper\n",
    "    \"\"\"\n",
    "    word_count = 0\n",
    "    if words== 'all':\n",
    "        word_row = word_hist_union_raw_df[word_hist_union_raw_df['Data Name'] == float(paper_id)].values.flatten()\n",
    "        word_count += int(np.sum(word_row[1:]))\n",
    "    else:\n",
    "        for word in words:\n",
    "            word_count += int(word_hist_union_raw_df[word_hist_union_raw_df['Data Name'] == float(paper_id)][word].values[0])\n",
    "    return word_count\n",
    "\n",
    "def get_word_count_for_author(author, words='all'):\n",
    "    \"\"\"\n",
    "        Gets the total word count for a list of words, if 'all' it will get the total word count of this author\n",
    "    \"\"\"\n",
    "    word_count = 0\n",
    "    paper_ids = get_paper_ids_for_author(author)\n",
    "    if len(paper_ids) == 0:\n",
    "        print(f'Error: could not find papers for {author}')\n",
    "    else:\n",
    "        for paper in paper_ids:\n",
    "            word_count += get_word_count_for_paper(paper, words=words)\n",
    "    return int(word_count)\n",
    "\n",
    "def total_words_by_author():\n",
    "    author_total_words_dict = {}\n",
    "    for author in list_of_authors:\n",
    "        author_total_words_dict[author] = get_word_count_for_author(author)\n",
    "    return author_total_words_dict\n",
    "    \n",
    "# We should remove the authors with a total word count of zero:\n",
    "\n",
    "def remove_authors(author_list):\n",
    "    new_metadata_df = metadata_df\n",
    "    for author in author_list:\n",
    "        i = new_metadata_df[new_metadata_df.first_author==author].index\n",
    "        new_metadata_df = new_metadata_df.drop(i, axis=0)\n",
    "    return new_metadata_df\n",
    "\n",
    "metadata_df = remove_authors(['Violeta Gamez Rosas', 'Emma P. Lieb'])\n",
    "list_of_authors = list(set(metadata_df['first_author']))\n",
    "#print(list_of_authors)\n",
    "\n",
    "word_totals = total_words_by_author()\n",
    "##print(word_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041268ea-b5d1-46a4-94f7-092c8e7c107b",
   "metadata": {},
   "source": [
    "Now we look at the relative word frequencies of each author for all words which occur in every paper (word_hist_inter_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ce92d-458e-4dfa-8b70-472241de9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intersection_words = list(word_hist_inter_raw_df)[1:]\n",
    "n_words = len(all_intersection_words)\n",
    "#print(all_intersection_words)\n",
    "\n",
    "def get_frequency_dict(word):\n",
    "    frequency_dict = {}\n",
    "    for author, value in word_totals.items():\n",
    "        if value != 0:\n",
    "            frequency_dict[author] = get_word_count_for_author(author, words=[word])/value\n",
    "    return frequency_dict\n",
    "\n",
    "def plot_word_frequency_per_author(word):\n",
    "    frequency_dict = get_frequency_dict(word)\n",
    "    X = np.array(list(frequency_dict.keys()))\n",
    "    Y = np.array(list(frequency_dict.values()))\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Relative word frequency [%]\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f\"Relative word frequency of '{word}' per author\")\n",
    "    bars = plt.bar(X, Y, width=0.5)\n",
    "    plt.show()\n",
    "\n",
    "#for word in all_intersection_words:\n",
    "plot_word_frequency_per_author('are')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db470e4-b519-433f-b30f-f3dc12fcff7f",
   "metadata": {},
   "source": [
    "Investigating the relative word frequency for each paper, grouped by author (for inter vs intra group variance analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55c4d1-07c9-4eaa-beaa-ecce286e19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, remove all authors with < n papers:\n",
    "\n",
    "def authors_with_min_req_papers(n=3):\n",
    "    \"\"\"\n",
    "    returns dictionary with authors who have written at least n papers as first author and the corresponding papers\n",
    "    \"\"\"\n",
    "    author_paper_dict = {}\n",
    "    for author in list_of_authors:\n",
    "        paper_ids = get_paper_ids_for_author(author)\n",
    "        if len(paper_ids) >= n:\n",
    "            author_paper_dict[author] = paper_ids\n",
    "    return author_paper_dict\n",
    "\n",
    "author_paper_dct = authors_with_min_req_papers(n=1)\n",
    "\n",
    "# now, plot the word frequencies of papers grouped per author\n",
    "def plot_word_frequency_per_paper_grouped(word):\n",
    "    i = 1\n",
    "    for author, papers in author_paper_dct.items():\n",
    "        n_papers = len(papers)\n",
    "        X = [i] * n_papers\n",
    "        Y = [get_word_count_for_paper(paper_id, words=[word])/get_word_count_for_paper(paper_id, words='all') for paper_id in papers]\n",
    "        plt.plot(X,Y, 'x')\n",
    "        i+=1\n",
    "    plt.show()\n",
    "\n",
    "#plot_word_frequency_per_paper_grouped('any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6032405-5dd6-4e2a-8b7d-e406e74a3f09",
   "metadata": {},
   "source": [
    "Investigating how distinct the usage of a word by a specific author is, assuming gaussian distribution NOTE: can we assume gaussian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38574c89-f2d8-4613-ab28-a934399d9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_histogram(word):\n",
    "    frequency_dict = get_frequency_dict(word)\n",
    "    plt.hist(np.array(list(frequency_dict.values())), bins=20)\n",
    "\n",
    "def estimate_word_dist(word):\n",
    "    frequency_dict = get_frequency_dict(word)\n",
    "    frequencies = np.array(list(frequency_dict.values()))\n",
    "    mu = frequencies.mean()\n",
    "    var = (frequencies - mu).T @ (frequencies - mu)/frequencies.size\n",
    "\n",
    "    #sigma = math.sqrt(var)\n",
    "    #x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    #plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "    return mu, var\n",
    "    \n",
    "#freq_histogram('in')\n",
    "#estimate_word_dist('and')\n",
    "\n",
    "def p(word, author):\n",
    "    mu, var = estimate_word_dist(word)\n",
    "    frequency = get_frequency_dict(word)[author]\n",
    "    return stats.norm.pdf(frequency, mu, math.sqrt(var))\n",
    "\n",
    "def find_most_special_word(author):\n",
    "    \"\"\"\n",
    "    finds the word out of all common words which is the most 'unlikely' to have the frequency this author uses it in\n",
    "    'Out of all of the common words [author] uses, which one is used the most untypical way?'\n",
    "    TODO: exclude author from distribution first (?)\n",
    "    \"\"\"\n",
    "    word_p_dict = {}\n",
    "    for word in all_intersection_words:\n",
    "        p_val = p(word, author)\n",
    "        word_p_dict[word] = p_val\n",
    "    word_result = min(word_p_dict, key=word_p_dict.get)\n",
    "    print(f\"Most special word for {author} is '{word_result}' with p = {word_p_dict[word_result]}\")\n",
    "    return word_result\n",
    "\n",
    "#sp_word = find_most_special_word('Zexuan Wu')\n",
    "#plot_word_frequency_per_author(sp_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a472fdd-054d-45b9-9a60-7d8bebfc7a15",
   "metadata": {},
   "source": [
    "For each feature (common word counts, syllable counts, sentence length and stdev, and easy word ratio) calculate the pairwise jensen-shannon divergence (base 2) and visualize the result. High pairwise divergences indicate a feature, which can distinguish two groups well. \n",
    "\n",
    "Grouping is performed along the categories: \"first_author\", \"primary_category\" and \"first_author_country\". If metadata for one of these categories is not available for a paper, the paper is ignored. \n",
    "\n",
    "Groups with fewer than 5 samples are ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2081b6-b420-40dd-a99a-983e6bf49ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.analysis as analysis\n",
    "result_dfs = analysis.feature_analysis_pipe()\n",
    "for group_name in result_dfs.keys():\n",
    "    result_df = result_dfs[group_name]\n",
    "    analysis.visualize_feature_analysis(result_df, group_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-lit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
