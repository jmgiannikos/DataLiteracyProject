{
  "arxiv_id": "2507.06813",
  "found": true,
  "title": "Intrinsic Training Signals for Federated Learning Aggregation",
  "authors": [
    "Cosimo Fiorini",
    "Matteo Mosconi",
    "Pietro Buzzega",
    "Riccardo Salami",
    "Simone Calderara"
  ],
  "first_author": "Cosimo Fiorini",
  "summary": "Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. While existing approaches for aggregating client-specific classification heads and adapted backbone parameters require architectural modifications or loss function changes, our method uniquely leverages intrinsic training signals already available during standard optimization. We present LIVAR (Layer Importance and VARiance-based merging), which introduces: i) a variance-weighted classifier aggregation scheme using naturally emergent feature statistics, and ii) an explainability-driven LoRA merging technique based on SHAP analysis of existing update parameter patterns. Without any architectural overhead, LIVAR achieves state-of-the-art performance on multiple benchmarks while maintaining seamless integration with existing FL methods. This work demonstrates that effective model merging can be achieved solely through existing training signals, establishing a new paradigm for efficient federated model aggregation. The code is available at https://github.com/aimagelab/fed-mammoth.",
  "primary_category": "cs.LG",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2025-07-09T13:03:23+00:00",
  "updated": "2025-09-15T10:32:32+00:00",
  "doi": null,
  "journal_ref": null,
  "comment": null,
  "pdf_url": "https://arxiv.org/pdf/2507.06813v2",
  "entry_id": "http://arxiv.org/abs/2507.06813v2"
}