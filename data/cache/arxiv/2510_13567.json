{
  "arxiv_id": "2510.13567",
  "found": true,
  "title": "DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning",
  "authors": [
    "Omayma Moussadek",
    "Riccardo Salami",
    "Simone Calderara"
  ],
  "first_author": "Omayma Moussadek",
  "summary": "Federated continual learning (FCL) enables models to learn new tasks across multiple distributed clients, protecting privacy and without forgetting previously acquired knowledge. However, current methods face challenges balancing performance, privacy preservation, and communication efficiency. We introduce a Distributed Online LoRA for Federated INcremental learning method DOLFIN, a novel approach combining Vision Transformers with low-rank adapters designed to efficiently and stably learn new tasks in federated environments. Our method leverages LoRA for minimal communication overhead and incorporates DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet heterogeneity settings, DOLFIN consistently surpasses six strong baselines in final average accuracy while matching their memory footprint. Orthogonal low-rank adapters offer an effective and scalable solution for privacy-preserving continual learning in federated settings.",
  "primary_category": "cs.LG",
  "categories": [
    "cs.LG"
  ],
  "published": "2025-10-15T14:07:49+00:00",
  "updated": "2025-10-15T14:07:49+00:00",
  "doi": null,
  "journal_ref": null,
  "comment": null,
  "pdf_url": "https://arxiv.org/pdf/2510.13567v1",
  "entry_id": "http://arxiv.org/abs/2510.13567v1"
}